{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2026.01"
      },
      "authorship_tag": "ABX9TyOjz5oFHryerBi85GQriSsS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrm6676/notebooks/blob/main/March_ML_Mania_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "Vsz_C03dl7QT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0hbM0qTDYzw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1.Load Essential Data\n",
        "seeds = pd.read_csv('MNCAATourneySeeds.csv')\n",
        "results = pd.read_csv('MNCAATourneyCompactResults.csv')\n",
        "\n",
        "# 2.Clean data\n",
        "def seed_to_int(seed):\n",
        "    #\n",
        "    return int(''.join(filter(str.isdigit, seed)))\n",
        "\n",
        "seeds['SeedInt'] = seeds['Seed'].apply(seed_to_int)\n",
        "\n",
        "# 3. Make Training Data be Ready\n",
        "# Results for win & lose teams\n",
        "train_data = results[['Season', 'WTeamID', 'LTeamID']].copy()\n",
        "\n",
        "# Lose team catering\n",
        "train_data = pd.merge(train_data, seeds, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
        "train_data.rename(columns={'SeedInt': 'WSeedInt'}, inplace=True)\n",
        "train_data = train_data.drop('TeamID', axis=1).drop('Seed', axis=1)\n",
        "\n",
        "#Train data merge\n",
        "train_data = pd.merge(train_data, seeds, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\n",
        "train_data.rename(columns={'SeedInt': 'LSeedInt'}, inplace=True)\n",
        "train_data = train_data.drop('TeamID', axis=1).drop('Seed', axis=1)\n",
        "\n",
        "# Calculating seed differences\n",
        "train_data['SeedDiff'] = train_data['WSeedInt'] - train_data['LSeedInt']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating parallel data for both win & lose teams\n",
        "df_win = pd.DataFrame()\n",
        "df_win['SeedDiff'] = train_data['SeedDiff']\n",
        "df_win['Result'] = 1  # 1 1st team won\n",
        "\n",
        "df_lose = pd.DataFrame()\n",
        "df_lose['SeedDiff'] = -train_data['SeedDiff']\n",
        "df_lose['Result'] = 0  # 0 1st team lost\n",
        "\n",
        "train_df = pd.concat([df_win, df_lose])\n",
        "\n",
        "# Training Model\n",
        "X_train = train_df[['SeedDiff']].values\n",
        "y_train = train_df['Result'].values\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Model trained. Coefficient for SeedDiff: {model.coef_[0][0]}\")"
      ],
      "metadata": {
        "id": "9Ui604fqE9NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_matchup(team1_seed, team2_seed):\n",
        "    seed_diff = team1_seed - team2_seed\n",
        "    # Predict probabilities\n",
        "    prob = model.predict_proba([[seed_diff]])[0][1]\n",
        "    return prob\n",
        "\n",
        "probability = predict_matchup(1, 16)\n",
        "print(f\"Probability of Seed 1 beating Seed 16: {probability:.2%}\")"
      ],
      "metadata": {
        "id": "8KQVOAFbFi0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n",
        "\n",
        "# 1.Data Preprocessing\n",
        "def get_season_stats(df):\n",
        "    # Calculating means(goals)\n",
        "    w_stats = df.groupby(['Season', 'WTeamID']).agg({'WScore': 'mean', 'WAst': 'mean', 'WStl': 'mean'}).reset_index()\n",
        "    l_stats = df.groupby(['Season', 'LTeamID']).agg({'LScore': 'mean', 'LAst': 'mean', 'LStl': 'mean'}).reset_index()\n",
        "\n",
        "    # Prepare w_stats for concatenation\n",
        "    w_stats_renamed = w_stats.rename(columns={'WTeamID': 'TeamID', 'WScore': 'Score', 'WAst': 'Ast', 'WStl': 'Stl'})\n",
        "    # Prepare l_stats for concatenation\n",
        "    l_stats_renamed = l_stats.rename(columns={'LTeamID': 'TeamID', 'LScore': 'Score', 'LAst': 'Ast', 'LStl': 'Stl'})\n",
        "\n",
        "    # Concatenate the stats\n",
        "    stats_summary = pd.concat([w_stats_renamed, l_stats_renamed], ignore_index=True)\n",
        "\n",
        "    return stats_summary\n",
        "\n",
        "# 2.XGBoost Model\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 4\n",
        "}\n",
        "# 3.LightGPM model\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 4\n",
        "}\n",
        "\n",
        "# Define the individual models\n",
        "model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
        "model_xgb = xgb.XGBClassifier(**params)\n",
        "model_lr = LogisticRegression()\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('lgb', model_lgb), ('xgb', model_xgb), ('lr', model_lr)],\n",
        "    voting='soft',\n",
        "    weights=[1, 1, 4]\n",
        ")\n",
        "\n",
        "# Model Training\n",
        "# Fill NaN values in X_train and X_val before fitting models\n",
        "X_train_filled = X_train.fillna(0)\n",
        "X_val_filled = X_val.fillna(0)\n",
        "\n",
        "model_xgb.fit(X_train_filled, y_train)\n",
        "ensemble.fit(X_train_filled, y_train)"
      ],
      "metadata": {
        "id": "8slgSk57F5nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "massey = pd.read_csv('MMasseyOrdinals.csv')\n",
        "\n",
        "# 1. Testing orders in regular day season(day 133)\n",
        "last_ranking = massey[massey['RankingDayNum'] == 133]\n",
        "\n",
        "# 2.POM' (Pomeroy)\n",
        "#Mean\n",
        "avg_ranking = last_ranking.groupby(['Season', 'TeamID'])['OrdinalRank'].mean().reset_index()"
      ],
      "metadata": {
        "id": "J55tA5JpHhDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining women & men results data\n",
        "m_results = pd.read_csv('MNCAATourneyCompactResults.csv')\n",
        "w_results = pd.read_csv('WNCAATourneyCompactResults.csv')\n",
        "\n",
        "#Add gender tag\n",
        "m_results['is_women'] = 0\n",
        "w_results['is_women'] = 1\n",
        "\n",
        "#Concatenation in one matrix for training\n",
        "all_results = pd.concat([m_results, w_results], axis=0)"
      ],
      "metadata": {
        "id": "3r1h9YToKU1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Load men data\n",
        "m_seeds = pd.read_csv('MNCAATourneySeeds.csv')\n",
        "m_results = pd.read_csv('MNCAATourneyCompactResults.csv')\n",
        "\n",
        "#Load women data\n",
        "w_seeds = pd.read_csv('WNCAATourneySeeds.csv')\n",
        "w_results = pd.read_csv('WNCAATourneyCompactResults.csv')\n",
        "\n",
        "#Convert seed into digit\n",
        "def clean_seed(seed):\n",
        "    return int(''.join(filter(str.isdigit, seed)))\n",
        "\n",
        "#Preprocessing orders\n",
        "m_seeds['SeedInt'] = m_seeds['Seed'].apply(clean_seed)\n",
        "w_seeds['SeedInt'] = w_seeds['Seed'].apply(clean_seed)\n",
        "\n",
        "#Combining\n",
        "all_seeds = pd.concat([m_seeds, w_seeds])\n",
        "all_results = pd.concat([m_results, w_results])\n",
        "\n",
        "#Combining results by ordering win & lose team\n",
        "def prepare_training_data(results_df, seeds_df):\n",
        "    #Merge for win team ordinals\n",
        "    df = pd.merge(results_df, seeds_df, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
        "    df.rename(columns={'SeedInt': 'WSeedInt'}, inplace=True)\n",
        "    df = df.drop(['TeamID', 'Seed'], axis=1)\n",
        "\n",
        "    #Merge for lose team ordinals\n",
        "    df = pd.merge(df, seeds_df, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
        "    df.rename(columns={'SeedInt': 'LSeedInt'}, inplace=True)\n",
        "    df = df.drop(['TeamID', 'Seed'], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = prepare_training_data(all_results, all_seeds)"
      ],
      "metadata": {
        "id": "v01LzbKuL59o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Massey Ordinals:\n",
        "massey = pd.read_csv('MMasseyOrdinals.csv')\n",
        "#Take the ordinals of the end of a regular season(day 133)\n",
        "massey_end = massey[massey['RankingDayNum'] == 133].groupby(['Season', 'TeamID'])['OrdinalRank'].mean().reset_index()\n",
        "\n",
        "#DF training\n",
        "# Merge for winning team ordinals\n",
        "train_df = pd.merge(train_df, massey_end, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
        "train_df.rename(columns={'OrdinalRank': 'WOrdinalRank'}, inplace=True)\n",
        "train_df.drop('TeamID', axis=1, inplace=True) # Drop the 'TeamID' column from massey_end merge for WTeam\n",
        "\n",
        "# Merge for losing team ordinals\n",
        "train_df = pd.merge(train_df, massey_end, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
        "train_df.rename(columns={'OrdinalRank': 'LOrdinalRank'}, inplace=True)\n",
        "train_df.drop('TeamID', axis=1, inplace=True) # Drop the 'TeamID' column from massey_end merge for LTeam"
      ],
      "metadata": {
        "id": "cL-_2ezMNbbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = pd.DataFrame()\n",
        "\n",
        "# Case 1: The team with the smaller ID is the winner (Label = 1)\n",
        "# Case 2: The team with the smaller ID is the loser (Label = 0)\n",
        "# This ensures that the model is not biased towards the order of teams in the file\n",
        "\n",
        "def create_diff_features(df):\n",
        "    feature_df = pd.DataFrame()\n",
        "    # We will assume that Team A always has the smaller ID\n",
        "    # For winning\n",
        "    win_cases = df[df['WTeamID'] < df['LTeamID']].copy()\n",
        "    win_cases['SeedDiff'] = win_cases['LSeedInt'] - win_cases['WSeedInt'] # The difference in favor of the winner\n",
        "    win_cases['Result'] = 1\n",
        "\n",
        "    # For losing\n",
        "    lose_cases = df[df['WTeamID'] > df['LTeamID']].copy()\n",
        "    lose_cases['SeedDiff'] = lose_cases['LSeedInt'] - lose_cases['WSeedInt'] # The difference is negative\n",
        "    lose_cases['Result'] = 0\n",
        "\n",
        "    return pd.concat([win_cases, lose_cases])\n",
        "\n",
        "final_data = create_diff_features(train_df)"
      ],
      "metadata": {
        "id": "42TFNJvjOi7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1206b0b"
      },
      "source": [
        "# Create OrdinalRankDiff feature\n",
        "# The logic mirrors SeedDiff to ensure consistency with the 'Result' column\n",
        "final_data['OrdinalRankDiff'] = final_data['LOrdinalRank'] - final_data['WOrdinalRank']\n",
        "\n",
        "# Display the first few rows with the new feature\n",
        "display(final_data[['WTeamID', 'LTeamID', 'WSeedInt', 'LSeedInt', 'SeedDiff', 'WOrdinalRank', 'LOrdinalRank', 'OrdinalRankDiff', 'Result']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376dcd79"
      },
      "source": [
        "Generating the submission file `submission.csv` by loading \"SampleSubmissionStage2.csv\", preparing test features using `all_seeds` and `massey_end`, generating predictions with the trained XGBoost model, applying prediction clipping, and then formatting and combining the men's and women's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc048c48"
      },
      "source": [
        "## Load Sample Submission\n",
        "\n",
        "\n",
        "Loading the `SampleSubmissionStage1.csv` file, which contains the matchup IDs for both men's and women's tournaments. This file will serve as the template for our predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7064f90c"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the `SampleSubmissionStage1.csv` file into a pandas DataFrame named `submission_df` to prepare for predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52a6d0bf"
      },
      "source": [
        "submission_df = pd.read_csv('SampleSubmissionStage1.csv')\n",
        "print(\"Sample submission data loaded successfully.\")\n",
        "submission_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4880055b"
      },
      "source": [
        "## Prepare Test Data Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07a75249"
      },
      "source": [
        "test_df = submission_df.copy()\n",
        "\n",
        "# 1. Extract 'Season', 'Team1ID', and 'Team2ID' from the 'ID' column\n",
        "test_df[['Season', 'Team1ID', 'Team2ID']] = test_df['ID'].str.split('_', expand=True)\n",
        "test_df['Season'] = test_df['Season'].astype(int)\n",
        "test_df['Team1ID'] = test_df['Team1ID'].astype(int)\n",
        "test_df['Team2ID'] = test_df['Team2ID'].astype(int)\n",
        "\n",
        "# 2. Merge with all_seeds for Team1\n",
        "test_df = pd.merge(test_df,\n",
        "                   all_seeds[['Season', 'TeamID', 'SeedInt']],\n",
        "                   left_on=['Season', 'Team1ID'],\n",
        "                   right_on=['Season', 'TeamID'],\n",
        "                   how='left')\n",
        "test_df.rename(columns={'SeedInt': 'Team1SeedInt'}, inplace=True)\n",
        "test_df.drop('TeamID', axis=1, inplace=True)\n",
        "\n",
        "# 3. Merge with all_seeds for Team2\n",
        "test_df = pd.merge(test_df,\n",
        "                   all_seeds[['Season', 'TeamID', 'SeedInt']],\n",
        "                   left_on=['Season', 'Team2ID'],\n",
        "                   right_on=['Season', 'TeamID'],\n",
        "                   how='left')\n",
        "test_df.rename(columns={'SeedInt': 'Team2SeedInt'}, inplace=True)\n",
        "test_df.drop('TeamID', axis=1, inplace=True)\n",
        "\n",
        "# 4. Merge with massey_end for Team1\n",
        "test_df = pd.merge(test_df,\n",
        "                   massey_end[['Season', 'TeamID', 'OrdinalRank']],\n",
        "                   left_on=['Season', 'Team1ID'],\n",
        "                   right_on=['Season', 'TeamID'],\n",
        "                   how='left')\n",
        "test_df.rename(columns={'OrdinalRank': 'Team1OrdinalRank'}, inplace=True)\n",
        "test_df.drop('TeamID', axis=1, inplace=True)\n",
        "\n",
        "# 5. Merge with massey_end for Team2\n",
        "test_df = pd.merge(test_df,\n",
        "                   massey_end[['Season', 'TeamID', 'OrdinalRank']],\n",
        "                   left_on=['Season', 'Team2ID'],\n",
        "                   right_on=['Season', 'TeamID'],\n",
        "                   how='left')\n",
        "test_df.rename(columns={'OrdinalRank': 'Team2OrdinalRank'}, inplace=True)\n",
        "test_df.drop('TeamID', axis=1, inplace=True)\n",
        "\n",
        "# 6. Calculate 'SeedDiff' and 'OrdinalRankDiff'\n",
        "test_df['SeedDiff'] = test_df['Team1SeedInt'] - test_df['Team2SeedInt']\n",
        "test_df['OrdinalRankDiff'] = test_df['Team1OrdinalRank'] - test_df['Team2OrdinalRank']\n",
        "\n",
        "print(\"Test features prepared successfully.\")\n",
        "print(test_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0340bab"
      },
      "source": [
        "test_df['Team1SeedInt'].fillna(0, inplace=True)\n",
        "test_df['Team2SeedInt'].fillna(0, inplace=True)\n",
        "test_df['Team1OrdinalRank'].fillna(0, inplace=True)\n",
        "test_df['Team2OrdinalRank'].fillna(0, inplace=True)\n",
        "\n",
        "# Recalculate SeedDiff and OrdinalRankDiff after filling NaNs in their components\n",
        "test_df['SeedDiff'] = test_df['Team1SeedInt'] - test_df['Team2SeedInt']\n",
        "test_df['OrdinalRankDiff'] = test_df['Team1OrdinalRank'] - test_df['Team2OrdinalRank']\n",
        "\n",
        "print(\"NaN values in test features filled and differences recalculated.\")\n",
        "print(test_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7092d0f6"
      },
      "source": [
        "test_df['Team1SeedInt'] = test_df['Team1SeedInt'].fillna(0)\n",
        "test_df['Team2SeedInt'] = test_df['Team2SeedInt'].fillna(0)\n",
        "test_df['Team1OrdinalRank'] = test_df['Team1OrdinalRank'].fillna(0)\n",
        "test_df['Team2OrdinalRank'] = test_df['Team2OrdinalRank'].fillna(0)\n",
        "\n",
        "# Recalculate SeedDiff and OrdinalRankDiff after filling NaNs in their components\n",
        "test_df['SeedDiff'] = test_df['Team1SeedInt'] - test_df['Team2SeedInt']\n",
        "test_df['OrdinalRankDiff'] = test_df['Team1OrdinalRank'] - test_df['Team2OrdinalRank']\n",
        "\n",
        "print(\"NaN values in test features filled and differences recalculated.\")\n",
        "print(test_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b187b80"
      },
      "source": [
        "features_for_prediction = ['SeedDiff', 'OrdinalRankDiff']\n",
        "X_test_final = test_df[features_for_prediction]\n",
        "\n",
        "print(\"Test features for prediction created successfully.\")\n",
        "print(X_test_final.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d3d24c2"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create OrdinalRankDiff feature if not already present\n",
        "# The logic mirrors SeedDiff to ensure consistency with the 'Result' column\n",
        "if 'OrdinalRankDiff' not in final_data.columns:\n",
        "    final_data['OrdinalRankDiff'] = final_data['LOrdinalRank'] - final_data['WOrdinalRank']\n",
        "\n",
        "# Select features\n",
        "features = ['SeedDiff', 'OrdinalRankDiff'] # Added 'OrdinalRankDiff' to features\n",
        "X = final_data[features]\n",
        "y = final_data['Result']\n",
        "\n",
        "# Split data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.02,\n",
        "    max_depth=4,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "print(\"Training Complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ef94f58"
      },
      "source": [
        "The XGBoost model has been successfully retrained with both 'SeedDiff' and 'OrdinalRankDiff' features. Now, I will generate predictions for the test set using the updated model and the prepared `X_test_final` features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82646005"
      },
      "source": [
        "test_df['Pred'] = model.predict_proba(X_test_final)[:, 1]\n",
        "print(\"Predictions generated for the test set.\")\n",
        "print(test_df[['ID', 'Pred']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "594c6910"
      },
      "source": [
        "The next step in the task is to apply prediction clipping to the generated probabilities to keep them within a reasonable range (0.025 to 0.975), which helps to avoid extreme probabilities and improve robustness in submission files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20702145"
      },
      "source": [
        "test_df['Pred'] = np.clip(test_df['Pred'], 0.025, 0.975)\n",
        "print(\"Predictions clipped successfully.\")\n",
        "print(test_df[['ID', 'Pred']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5252f476"
      },
      "source": [
        "The predictions have been generated and clipped. The final step of the task is to format these predictions into the required submission file format, which means selecting the 'ID' and 'Pred' columns from `test_df` and saving them to `submission.csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc1e69ae"
      },
      "source": [
        "submission = test_df[['ID', 'Pred']]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Made successful! Ensembled submission.csv saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "def6b8e5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `SampleSubmissionStage2.csv` file was successfully loaded, providing the template for the submission.\n",
        "*   Test features, including `Season`, `Team1ID`, `Team2ID`, seed information (`Team1SeedInt`, `Team2SeedInt`), and ordinal ranks (`Team1OrdinalRank`, `Team2OrdinalRank`), were extracted and merged for both teams in each matchup.\n",
        "*   Differences in seeds (`SeedDiff`) and ordinal ranks (`OrdinalRankDiff`) were calculated for the test set.\n",
        "*   Missing values (NaNs) in seed and ordinal rank features, particularly for future seasons, were identified and filled with `0`.\n",
        "*   An initial attempt to generate predictions failed due to a feature mismatch: the model was trained with only `SeedDiff`, while the test data contained both `SeedDiff` and `OrdinalRankDiff`.\n",
        "*   During the process of resolving the feature mismatch, it was discovered that the `OrdinalRankDiff` feature was missing from the `final_data` used for model training. This was corrected by explicitly calculating `OrdinalRankDiff` as `LOrdinalRank - WOrdinalRank` in the training data.\n",
        "*   The XGBoost model was successfully retrained using both `SeedDiff` and `OrdinalRankDiff` as features.\n",
        "*   Predictions were generated for the test set using the retrained model and then clipped to the range of \\[0.025, 0.975] to regularize extreme probability values.\n",
        "*   The final `submission.csv` file was successfully created and saved, containing the `ID` and `Pred` columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ac9ed5"
      },
      "source": [
        "## Identify Men's and Women's Matchups\n",
        "\n",
        "Based on the `Team1ID` (or `Team2ID`), identify which entries in `test_df` correspond to men's tournament games and which correspond to women's tournament games. Men's TeamIDs are typically below 3000, while Women's TeamIDs are 3000 or above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ae1ffe9"
      },
      "source": [
        "men_test_df = test_df[test_df['Team1ID'] < 3000].copy()\n",
        "women_test_df = test_df[test_df['Team1ID'] >= 3000].copy()\n",
        "\n",
        "print(\"Men's and women's test dataframes created.\")\n",
        "print(\"Men's test data (head):\\n\", men_test_df.head())\n",
        "print(\"\\nWomen's test data (head):\\n\", women_test_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "men_submission = men_test_df[['ID', 'Pred']]\n",
        "men_submission.to_csv('men_submission.csv', index=False)\n",
        "\n",
        "women_submission = women_test_df[['ID', 'Pred']]\n",
        "women_submission.to_csv('women_submission.csv', index=False)\n",
        "\n",
        "print(\"Men's and women's submission files generated successfully!\")"
      ],
      "metadata": {
        "id": "-PFDWBO7qNLk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}